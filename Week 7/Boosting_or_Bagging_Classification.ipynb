{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNernGxGZ5GkvKX4oyg2Oua",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahrulrozi11/Machine-Learning/blob/main/Boosting_or_Bagging_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jyJ9wJBj_W9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.IMPORT LIB"
      ],
      "metadata": {
        "id": "6mMWgGSu_amq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vrArmn-p_VuF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Loading Dataset"
      ],
      "metadata": {
        "id": "4Ry2oLeq_eRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean_data(dataset_name):\n",
        "    def load_dataset(name, url, default_target):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Memproses dataset: {name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        df = pd.read_csv(url)\n",
        "        print(f\"Jumlah data: {df.shape[0]}\")\n",
        "        print(f\"Jumlah fitur: {df.shape[1] - 1}\")\n",
        "        print(f\"Nilai kosong: {df.isnull().sum().sum()}\")\n",
        "        print(\"\\nKolom dalam dataset:\", df.columns.tolist())\n",
        "\n",
        "        # Deteksi kolom target\n",
        "        target_col = next((col for col in [default_target, default_target.lower()] if col in df.columns), None)\n",
        "        if target_col is None:\n",
        "            target_col = next((col for col in df.columns if df[col].nunique() <= 5), df.columns[-1])\n",
        "\n",
        "        print(f\"\\nMenggunakan '{target_col}' sebagai kolom target\")\n",
        "        print(\"Distribusi Target:\")\n",
        "        print(df[target_col].value_counts(normalize=True))\n",
        "\n",
        "        # Ubah target menjadi numerik jika belum\n",
        "        if not pd.api.types.is_numeric_dtype(df[target_col]):\n",
        "            df[target_col] = LabelEncoder().fit_transform(df[target_col])\n",
        "            print(\"\\nKolom target telah diubah menjadi nilai numerik\")\n",
        "\n",
        "        # Pisahkan fitur dan target\n",
        "        X = df.drop(columns=target_col)\n",
        "        y = df[target_col]\n",
        "\n",
        "        # Kategorisasi tipe kolom\n",
        "        numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        print(\"\\nFitur numerik:\", numerical_cols)\n",
        "        print(\"Fitur kategorikal:\", categorical_cols)\n",
        "\n",
        "        return X, y, numerical_cols, categorical_cols\n",
        "\n",
        "    # Peta dataset ke URL dan target default\n",
        "    dataset_map = {\n",
        "        'heart': (\"Heart Disease\",\n",
        "                  \"https://raw.githubusercontent.com/farrelrassya/teachingMLDL/main/01.%20Machine%20Learning/01.%20Week%201/Dataset/HeartDisease.csv\",\n",
        "                  \"target\"),\n",
        "        'water': (\"Citarum Water\",\n",
        "                  \"https://raw.githubusercontent.com/farrelrassya/teachingMLDL/main/01.%20Machine%20Learning/02.%20Week%202/Dataset/CitarumWater.csv\",\n",
        "                  \"Class\"),\n",
        "        'income': (\"Income\",\n",
        "                   \"https://raw.githubusercontent.com/farrelrassya/teachingMLDL/main/02.%20Deep%20Learning/Dataset/income.csv\",\n",
        "                   \"income\")\n",
        "    }\n",
        "\n",
        "    if dataset_name in dataset_map:\n",
        "        name, url, target = dataset_map[dataset_name]\n",
        "        return load_dataset(name, url, target)\n",
        "    else:\n",
        "        raise ValueError(\"Nama dataset tidak dikenali. Gunakan 'heart', 'water', atau 'income'.\")\n"
      ],
      "metadata": {
        "id": "6uFfvFWl_puq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocessing"
      ],
      "metadata": {
        "id": "CrX-Y5FPAAA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_preprocessing_pipeline(numerical_cols, categorical_cols):\n",
        "    # Langkah pra-pemrosesan untuk kolom numerik: imputasi dengan median, lalu normalisasi\n",
        "    numeric_pipeline = Pipeline([\n",
        "        ('isi_kosong', SimpleImputer(strategy='median')),\n",
        "        ('normalisasi', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Langkah pra-pemrosesan untuk kolom kategorikal: imputasi dengan modus, lalu one-hot encoding\n",
        "    categorical_pipeline = Pipeline([\n",
        "        ('isi_kosong', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoding', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Gabungkan pipeline numerik dan kategorikal ke dalam ColumnTransformer\n",
        "    preprocessing_pipeline = ColumnTransformer([\n",
        "        ('numerik', numeric_pipeline, numerical_cols),\n",
        "        ('kategori', categorical_pipeline, categorical_cols)\n",
        "    ])\n",
        "\n",
        "    return preprocessing_pipeline\n"
      ],
      "metadata": {
        "id": "w8aDirKsAAvI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Training & Evaluasi Model Boosting"
      ],
      "metadata": {
        "id": "ygDX_AYtALTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, preprocessor, model_name='xgboost'):\n",
        "    # Pilih model berdasarkan nama yang diberikan\n",
        "    if model_name == 'xgboost':\n",
        "        estimator = xgb.XGBClassifier(random_state=42)\n",
        "        tuning_params = {\n",
        "            'estimator__learning_rate': [0.01, 0.1],\n",
        "            'estimator__max_depth': [3, 5],\n",
        "            'estimator__n_estimators': [100, 200]\n",
        "        }\n",
        "    elif model_name == 'lightgbm':\n",
        "        estimator = lgb.LGBMClassifier(random_state=42)\n",
        "        tuning_params = {\n",
        "            'estimator__learning_rate': [0.01, 0.1],\n",
        "            'estimator__num_leaves': [31, 50],\n",
        "            'estimator__n_estimators': [100, 200]\n",
        "        }\n",
        "    else:\n",
        "        estimator = GradientBoostingClassifier(random_state=42)\n",
        "        tuning_params = {\n",
        "            'estimator__learning_rate': [0.01, 0.1],\n",
        "            'estimator__max_depth': [3, 5],\n",
        "            'estimator__n_estimators': [100, 200]\n",
        "        }\n",
        "\n",
        "    # Gabungkan preprocessing dan model ke dalam pipeline\n",
        "    full_pipeline = Pipeline([\n",
        "        ('preprocessing', preprocessor),\n",
        "        ('estimator', estimator)\n",
        "    ])\n",
        "\n",
        "    print(f\"\\n🔍 Menjalankan Grid Search untuk {model_name}...\")\n",
        "\n",
        "    # Lakukan pencarian parameter terbaik\n",
        "    search = GridSearchCV(\n",
        "        full_pipeline,\n",
        "        param_grid=tuning_params,\n",
        "        cv=3,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    search.fit(X_train, y_train)\n",
        "    print(f\"✅ Parameter terbaik: {search.best_params_}\")\n",
        "\n",
        "    # Ambil pipeline terbaik dari hasil pencarian\n",
        "    best_model = search.best_estimator_\n",
        "\n",
        "    # Prediksi data uji\n",
        "    predictions = best_model.predict(X_test)\n",
        "\n",
        "    # Jika klasifikasi biner, coba hitung AUC\n",
        "    try:\n",
        "        proba = best_model.predict_proba(X_test)[:, 1] if len(np.unique(y_test)) == 2 else None\n",
        "        auc = roc_auc_score(y_test, proba) if proba is not None else None\n",
        "    except:\n",
        "        proba = None\n",
        "        auc = None\n",
        "        print(\"⚠️ Gagal menghitung AUC. Model mungkin tidak mendukung predict_proba.\")\n",
        "\n",
        "    # Kumpulkan metrik evaluasi\n",
        "    eval_metrics = {\n",
        "        'accuracy': accuracy_score(y_test, predictions),\n",
        "        'precision': precision_score(y_test, predictions, average='weighted'),\n",
        "        'recall': recall_score(y_test, predictions, average='weighted'),\n",
        "        'f1': f1_score(y_test, predictions, average='weighted'),\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "    print(\"\\n📊 Hasil Evaluasi:\")\n",
        "    for key, val in eval_metrics.items():\n",
        "        if val is not None:\n",
        "            print(f\"{key}: {val:.4f}\")\n",
        "\n",
        "    print(\"\\n📄 Classification Report:\")\n",
        "    print(classification_report(y_test, predictions))\n",
        "\n",
        "    return best_model, eval_metrics, predictions, proba\n"
      ],
      "metadata": {
        "id": "dbGHAO8tALHM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisasi data"
      ],
      "metadata": {
        "id": "ZY4meJ4bAwmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_data(X, y):\n",
        "    # Histogram untuk fitur numerik\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(X[col], kde=True)\n",
        "        plt.title(f'Distribusi {col}')\n",
        "        plt.show()\n",
        "\n",
        "    # Boxplot untuk fitur numerik\n",
        "    for col in numerical_cols:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.boxplot(x=y, y=X[col])\n",
        "        plt.title(f'Boxplot {col} berdasarkan target')\n",
        "        plt.show()\n",
        "\n",
        "    # Countplot untuk fitur kategorikal\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    for col in categorical_cols:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.countplot(x=X[col], hue=y)\n",
        "        plt.title(f'Distribusi {col} berdasarkan target')\n",
        "        plt.xticks(rotation=45, ha='right')  # Rotasi label sumbu x untuk keterbacaan\n",
        "        plt.show()\n",
        "\n",
        "    # Heatmap korelasi fitur numerik\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(X[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Korelasi antar fitur numerik')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "DLFGoW6TBIVx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ":# Penjelasan Matematika: Boosting dan Metrik Evaluasi Klasifikasi\n",
        "\n",
        "## A. Boosting untuk Klasifikasi\n",
        "\n",
        "Boosting merupakan metode ensemble yang mengombinasikan beberapa model lemah (base learners) secara iteratif untuk membangun model yang lebih kuat. Secara umum, model boosting dapat dituliskan sebagai:\n",
        "$$\n",
        "F(x) = F_0(x) + \\sum_{m=1}^{M} \\gamma_m h_m(x)\n",
        "$$\n",
        "di mana:\n",
        "- $F_0(x)$ adalah prediksi awal (misalnya probabilitas dasar atau log odds),\n",
        "- $h_m(x)$ adalah base learner pada iterasi ke-$m$ (misalnya decision tree),\n",
        "- $\\gamma_m$ adalah bobot learning rate yang mengatur kontribusi masing-masing base learner.\n",
        "\n",
        "Pada setiap iterasi, boosting mengupdate model dengan memfokuskan pada error yang masih tersisa dari model sebelumnya.\n",
        "\n",
        "## B. Metrik Evaluasi Klasifikasi\n",
        "\n",
        "Evaluasi performa model klasifikasi dilakukan dengan beberapa metrik sebagai berikut:\n",
        "\n",
        "1. **Accuracy**  \n",
        "   $$\n",
        "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "   $$\n",
        "   Accuracy mengukur proporsi prediksi yang benar.\n",
        "\n",
        "2. **Precision**  \n",
        "   $$\n",
        "   \\text{Precision} = \\frac{TP}{TP+FP}\n",
        "   $$\n",
        "   Precision menunjukkan akurasi prediksi positif.\n",
        "\n",
        "3. **Recall**  \n",
        "   $$\n",
        "   \\text{Recall} = \\frac{TP}{TP+FN}\n",
        "   $$\n",
        "   Recall mengukur kemampuan model menangkap kasus positif.\n",
        "\n",
        "4. **F1 Score**  \n",
        "   F1 score adalah rata-rata harmonis dari precision dan recall:\n",
        "   $$\n",
        "   F1 = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "   $$\n",
        "   Untuk analisis tambahan, kita juga dapat menghitung **F1 Squared** sebagai:\n",
        "   $$\n",
        "   F1^2 = (F1)^2\n",
        "   $$\n",
        "\n",
        "5. **ROC Curve dan AUC**  \n",
        "   ROC (Receiver Operating Characteristic) menggambarkan trade-off antara True Positive Rate (Recall) dan False Positive Rate:\n",
        "   $$\n",
        "   \\text{TPR} = \\frac{TP}{TP+FN}, \\quad \\text{FPR} = \\frac{FP}{FP+TN}\n",
        "   $$\n",
        "   AUC (Area Under Curve) mengukur luas di bawah ROC curve sehingga semakin mendekati 1, semakin baik performa model.\n",
        "\n",
        "Metrik terbaik biasanya ditentukan berdasarkan konteks aplikasinya. Sebagai contoh, pada deteksi penyakit (misalnya HeartDisease) Recall seringkali menjadi metrik penting, sedangkan pada aplikasi lain (misalnya income prediction) AUC bisa memberikan gambaran yang lebih komprehensif terhadap performa model.\n"
      ],
      "metadata": {
        "id": "B9zkySZ0CFaL"
      }
    }
  ]
}